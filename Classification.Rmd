
---
title: "**Classification Methods**"
subtitle: "Decision Trees and KNN" 
author: Aisling Kinsella
date: "`r Sys.Date()`"
output: 
  pdf_document:
    latex_engine: xelatex
    toc: true
    toc_depth: 2
    number_sections: true
    df_print: kable
    fig_width: 7
    fig_height: 4
    fig_caption: true
    extra_dependencies: rotating
geometry: margin=1in
fontsize: 12pt
header-includes:
# - \usepackage[utf8]{inputenc}
- \usepackage{booktabs}
- \usepackage{float}
- \usepackage{dcolumn}
- \usepackage{fontspec}
- \setmainfont{Avenir Light} # GFS Didot
- \setsansfont{Raleway}

---


``` {r setup, include = FALSE}
knitr::opts_chunk$set(echo = FALSE, fig.showtext = TRUE)

# Load Libraries
library(knitr)
library(dplyr)
library(tidyverse)
library(kableExtra) 
library(pander)
library(viridis)
library(RColorBrewer)
library(gridExtra)
library(grid)
library(gtable)
library(showtext)
library(class)
library(gmodels)
library(caret)

library(rpart)
devtools:: install_github("cran/modiscloud")
library(rpart.plot)
library(rattle)

 # Load fonts 
font_add(family = "Proxima Nova Light", regular = "/Users/aislingkinsella/Library/Fonts/Proxima Nova Light.otf") 
font_add(family = "Proxima Nova Thin", regular = "/Users/aislingkinsella/Library/Fonts/Proxima Nova Thin.otf") 
font_add(family = "Proxima Nova Regular", regular = "/Users/aislingkinsella/Library/Fonts/Proxima Nova Regular.otf") 

options(xtable.comment = FALSE)

# to switch between latex font sizes from chunk to markdown text
def.chunk.hook  <- knitr::knit_hooks$get("chunk")
knitr::knit_hooks$set(chunk = function(x, options) {
  x <- def.chunk.hook(x, options)
  ifelse(options$size != "normalsize", paste0("\n \\", options$size,"\n\n", x, "\n\n \\normalsize"), x)
})
```

# Introduction

This work aims to classify mushrooms of the gilled species in the Agaricus and Lepiota Family into edible or poisonous. Two different classification methods are used alternately; decision trees and k-nearest neighbours. As there is no simple for rule for this type of segregation it is an appropriate dataset for machine learning algorithms. 


```{r importData, echo=FALSE}

# Load Datasets
mushroomData <- read.csv("data/agaricus-lepiota.data", stringsAsFactors = TRUE, header = FALSE, check.names = FALSE) 

# check dimensions
dfDim <- dim(mushroomData) 
#dfDim
#dfStr <- str(mushroomData)

```

## Dataset


The dataset was donated by Jeff Schlimmer to the UCI machine learning repository. It consists of `r dfDim[1]` records and `r dfDim[2]` variables. The dataset is shown below with factors labeled. 


```{r colNames,  echo = FALSE}

# convert to dataframe
mushroomDF <- data.frame(mushroomData)
# remove missing values
mushroomDF <- na.omit(mushroomDF)


# add col names
columns <- c("toxicity", "cap_shape", "cap_surface", "cap_color", "bruises", "odor", "gill_attach", "gill_spacing", "gill_size", "gill_color", "stalk_shape", "stalk_root", "stalk_surface_above", "stalk_surface_below", "stalk_color_above", "stalk_color_below", "veil_type", "veil_color", "ring_number", "ring_type", "spore_print_color", "population", "habitat")

names(mushroomDF) <- columns
```


```{r labelFact, echo = FALSE}

# Label factors

# toxicity 
mushroomDF <- transform(mushroomDF, toxicity = ifelse(toxicity == 'p', "poisonous", "edible"))

# 1. cap-shape: bell=b,conical=c,convex=x,flat=f, knobbed=k,sunken=s
mushroomDF <- transform(mushroomDF, cap_shape = ifelse(cap_shape == 'b', "bell", ifelse(cap_shape == 'c', "conical", ifelse(cap_shape == 'x', "convex", ifelse(cap_shape == 'f', "flat", ifelse(cap_shape == 'k', "knobbed", "sunken"))))) )

# 2. cap-surface: fibrous=f,grooves=g,scaly=y,smooth=s
mushroomDF <- transform(mushroomDF, cap_surface = ifelse(cap_surface == 'f', "fibrous", ifelse(cap_surface == 'g', "grooves", ifelse(cap_surface == 'y', "scaly", "smooth")))) 

# 3. cap-color: brown=n,buff=b,cinnamon=c,gray=g,green=r, pink=p,purple=u,red=e,white=w,yellow=y
mushroomDF <- transform(mushroomDF, cap_color = ifelse(cap_color == 'n', "brown", ifelse(cap_color == 'b', "buff", ifelse(cap_color == 'c', "cinnamon", ifelse(cap_color == 'g', "gray", ifelse(cap_color == 'r', "green", ifelse(cap_color == 'p', "pink", ifelse(cap_color == 'u', "purple", ifelse(cap_color == 'e', "red", ifelse(cap_color == 'w', "white", "yellow"))))))))))

# 4.  bruises=t,no=f
# mushroomDF <- transform(mushroomDF, bruises = ifelse(bruises == 't', "True", "False"))

# 5. odor: almond=a,anise=l,creosote=c,fishy=y,foul=f, musty=m,none=n,pungent=p,spicy=s
mushroomDF <- transform(mushroomDF, odor = ifelse(odor == 'a', "almond", ifelse(odor == 'l', "anise", ifelse(odor == 'c', "creosote", ifelse(odor  == 'y', "fishy", ifelse(odor == 'f', "foul", ifelse(odor == 'm', "musty", ifelse(odor == 'n', "none", ifelse(odor == 'p', "pungent", "spicy")))))))))

# 6. gill-attachment: attached=a,descending=d,free=f,notched=n
mushroomDF <- transform(mushroomDF, gill_attach = ifelse(gill_attach == 'a', "attached", ifelse(gill_attach == 'd', "descending", ifelse(gill_attach == 'f', "free", "notched"))))

# 7. gill-spacing: close=c,crowded=w,distant=d
mushroomDF <- transform(mushroomDF, gill_spacing = ifelse(gill_spacing == 'c', "close", ifelse(gill_spacing == 'w', "crowded", "distant"))) 

# 8. gill-size: broad=b,narrow=n
mushroomDF <- transform(mushroomDF, gill_size = ifelse(gill_size == 'b', "broad", "narrow"))

# 9. gill-color: black=k,brown=n,buff=b,chocolate=h,gray=g, green=r,orange=o,pink=p,purple=u,red=e, white=w,yellow=y
mushroomDF <- transform(mushroomDF, gill_color = ifelse(gill_color == 'k', "black", ifelse(gill_color == 'n', "brown", ifelse(gill_color == 'b', "buff", ifelse(gill_color  == 'h', "chocolate", ifelse(gill_color == 'g', "gray", ifelse(gill_color == 'r', "green", ifelse(gill_color == 'o', "orange", ifelse(gill_color == 'p', "pink", ifelse(gill_color == 'u', "purple", ifelse(gill_color == 'e', "red", ifelse(gill_color == 'w', "white", "yellow"))))))))))))


# 10. stalk-shape: enlarging=e,tapering=t
mushroomDF <- transform(mushroomDF, stalk_shape = ifelse(stalk_shape == 'e', "enlarging", "tapering"))

# 11. stalk-root: bulbous=b,club=c,cup=u,equal=e, rhizomorphs=z,rooted=r,missing=?
mushroomDF <- transform(mushroomDF, stalk_root =  ifelse(stalk_root == '?', "NA", ifelse(stalk_root == 'b', "bulbous", ifelse(stalk_root == 'c', "club", ifelse(stalk_root == 'u', "cup", ifelse(stalk_root  == 'e', "equal", ifelse(stalk_root == 'z', "rhizomorph", "rooted")))))))

# 12. stalk-surface-above-ring: fibrous=f,scaly=y,silky=k,smooth=s
mushroomDF <- transform(mushroomDF, stalk_surface_above = ifelse(stalk_surface_above == 'f', "fibrous", ifelse(stalk_surface_above == 'y', "scaly", ifelse(stalk_surface_above == 'k', "silky", "smooth")))) 

# 13. stalk-surface-below-ring: fibrous=f,scaly=y,silky=k,smooth=s
mushroomDF <- transform(mushroomDF, stalk_surface_below = ifelse(stalk_surface_below == 'f', "fibrous", ifelse(stalk_surface_below == 'y', "scaly", ifelse(stalk_surface_below == 'k', "silky", "smooth")))) 

# 14. stalk-color-above-ring: brown=n,buff=b,cinnamon=c,gray=g,orange=o, pink=p,red=e,white=w,yellow=y
mushroomDF <- transform(mushroomDF, stalk_color_above = ifelse(stalk_color_above == 'n', "brown", ifelse(stalk_color_above == 'b', "buff", ifelse(stalk_color_above == 'c', "cinnamon", ifelse(stalk_color_above == 'g', "gray", ifelse(stalk_color_above == 'o', "orange", ifelse(stalk_color_above == 'p', "pink", ifelse(stalk_color_above == 'e', "red", ifelse(stalk_color_above == 'w', "white", "yellow")))))))))

# 15. stalk-color-below-ring: brown=n,buff=b,cinnamon=c,gray=g,orange=o, pink=p,red=e,white=w,yellow=y
mushroomDF <- transform(mushroomDF, stalk_color_below = ifelse(stalk_color_below == 'n', "brown", ifelse(stalk_color_below == 'b', "buff", ifelse(stalk_color_below == 'c', "cinnamon", ifelse(stalk_color_below == 'g', "gray", ifelse(stalk_color_below == 'o', "orange", ifelse(stalk_color_below== 'p', "pink", ifelse(stalk_color_below == 'e', "red", ifelse(stalk_color_below == 'w', "white", "yellow")))))))))

# 16. veil-type: partial=p,universal=u
mushroomDF <- transform(mushroomDF, veil_type = ifelse(veil_type == 'p', "partial","universal")) 
                                                                    
# 17. veil-color: brown=n,orange=o,white=w,yellow=y
mushroomDF <- transform(mushroomDF, veil_color = ifelse(veil_color == 'n', "brown", ifelse(veil_color == 'o', "orange", ifelse(veil_color == 'w', "white", "yellow"))))

# 18. ring-number: none=n,one=o,two=t
mushroomDF <- transform(mushroomDF, ring_number = ifelse(ring_number == 'n', "none", ifelse(ring_number == 'o', "one", "two"))) 


# 19. ring-type: cobwebby=c,evanescent=e,flaring=f,large=l, none=n,pendant=p,sheathing=s,zone=z
mushroomDF <- transform(mushroomDF, ring_type = ifelse(ring_type == 'c', "cobwebby", ifelse(ring_type == 'e', "evanescent", ifelse(ring_type == 'f', "flaring", ifelse(ring_type  == 'l', "large", ifelse(ring_type == 'n', "none", ifelse(ring_type == 'p', "pendant", ifelse(ring_type == 's', "sheathing", "zone"))))))))

# 20. spore-print-color: black=k,brown=n,buff=b,chocolate=h,green=r, orange=o,purple=u,white=w,yellow=y
mushroomDF <- transform(mushroomDF, spore_print_color = ifelse(spore_print_color == 'k', "black", ifelse(spore_print_color == 'n', "brown", ifelse(spore_print_color == 'b', "buff", ifelse(spore_print_color  == 'h', "chocolate", ifelse(spore_print_color == 'r', "green", ifelse(spore_print_color == 'o', "orange", ifelse(spore_print_color == 'u', "purple", ifelse(spore_print_color == 'w', "white", "yellow")))))))))

# 21. population: abundant=a,clustered=c,numerous=n, scattered=s,several=v,solitary=y
mushroomDF <- transform(mushroomDF, population = ifelse(population == 'a', "abundant", ifelse(population == 'c', "clustered",                                      ifelse(population == 'n', "numerous", ifelse(population == 's', "scattered", ifelse(population == 'v', "several", "solitary"))))))

# 22. habitat: grasses=g,leaves=l,meadows=m,paths=p, urban=u,waste=w,woods=d
mushroomDF <- transform(mushroomDF, habitat = ifelse(habitat == 'g', "grasses", ifelse(habitat == 'l', "leaves",                                      ifelse(habitat == 'm', "meadows",
ifelse(habitat == 'p', "paths",
ifelse(habitat == 'u', "urban",
ifelse(habitat == 'w', "waste",
"woods")))))))      

```

\footnotesize
```{r viewHead, echo = FALSE}

# View Head
pander(head(mushroomDF), caption= "Mushroom Specifications\n")
```

_Origin:_
_Mushroom records drawn from The Audubon Society Field Guide to North American Mushrooms (1981). G. H. Lincoff (Pres.), New York: Alfred A. Knopf_

\normalsize

# Exploratory Data Analysis

## Factor Coding

All categorical variables must be factorised. Independent Varible *veil_type* is a factor type with only 1 level. This will be removed.

```{r factCo, echo = FALSE}


factorDF <- mushroomDF

# all character columns to factor:
factorDF <- mutate_if(factorDF, is.character, as.factor)
#str(factorDF)

# drop veil_type column as only one level in factor
factorDF = subset(factorDF, select = -c(17) )
```

## Encoding Categoricals

Using *one hot encoding*, columns are created for each unique value of each categorical variable. These columns are binary with a 1 or 0 if the factor is true or not. 


\footnotesize
```{r fac2Bin, echo = FALSE}

# encode factors to binary
encodedDF1 <- dummyVars(" ~ .", data = factorDF)
encodedDF2 <- data.frame(predict(encodedDF1, newdata = factorDF)) # must use predict function to create the variables

#str(encodedDF)

# print table
pander(head(encodedDF2[1:6], 6), caption= "Binary Encoded Categoricals: Sample")

#str(encodedDF2)
```

\normalsize

## Summary Statistics

Explore the distribution across the target variable:

```{r sum, echo = FALSE}
# view dist
pander(table(mushroomDF$toxicity), caption = "Distribution of Mushrooms Toxicity")
```


# Building the Model: Decision Tree

## Choosing Precision

The classifiers ability to not label poisonous mushrooms as edible is imperative in this instance. Precision is the measure of the model's ability to not mislabel in this way. In this case it is important to have precision of 100%

The measure of precision is True Positives/ True Positives + False Positives

## Partitioning the data

The data is vectorised for data paritioning and divided 80:20; 80 per cent appotioned to training the model, 20 per cent to testing the model.

```{r partition, echo = FALSE}

#head(encodedDF2)
#str(encodedDF2)

# str(encodedDF2$toxicity.poisonous)
# typeof(encodedDF2$toxicity.poisonous)

unlistedDF <- unlist_df(encodedDF2)
#head(unlistedDF)

# split the data into training and test set
set.seed(123) # randomise


### Error using unlisted factor to binary encoded data, return to original dataset and split

mushroomDF = subset(mushroomDF, select = -c(17) )# remove one factor column

training.samples <- mushroomDF$toxicity %>% 
  createDataPartition(p = 0.8, list = FALSE) 

train.data <- mushroomDF[training.samples,]
test.data <- mushroomDF[-training.samples,]

#dim(test.data)

```

\newpage
## Building the Model 

The model shows a poisonous classification 48% of the time, an edible classification 52% of the time. This appears close to the distribution revealed previously. 


```{r train, echo = FALSE}

model1 <- rpart(toxicity ~., data = mushroomDF, method = "class") 

# plot the tree
rpart.plot(model1, box.palette = "auto", 
extra = 104, # show fitted class, probs, percentages
branch.lty = 3, # dotted branch lines
shadow.col = "gray", # shadows under the node boxes
nn = TRUE, # display node numbers
cex = 0.7)
# fancyRpartPlot(model1, main = "Decision Tree", cex=0.8)

```



## Prediction

Prediction is made on the test data

```{r predict1, echo = FALSE}

predicted.classes <- model1 %>% predict(test.data, type = "class", na.omit)
#head(predicted.classes)

```

```{r modelEval, echo = FALSE}

# Compute model accuracy rate on test data
predictMean <- mean(predicted.classes == test.data$toxicity)
predictMean
```
## Evaluating the Model 

The overall accuracy of our tree model is 0.9975%. The tree is not complex, it is interpretable with a high accuracy rate. More levels in the tree, apart from presenting interpretablilty issues, raise the risk of model overfitting. 


## Improving  the Model

Improvements can be achieved by pruning the tree. The complexity parameter (*cp*) can be set to control the size of the tree if it were overly complex and lacking interpretability. The cp will ultimatey choose the optimal tree size and fit the model that explains our data the best.

Output from the complexity parameter table for the fitted data shows that the best cross validation error achievable is 0.012257. This can be fed into our model to prune the tree optimally. As the initial tree was performing well, there is no improvement in output.

\hfill\break
\hfill\break

```{r prune, echo = FALSE}
# pruning the tree 
pander(printcp(model1))

impModel1 <- prune(model1, cp = 0.012257) 

# plot the tree
rpart.plot(impModel1, box.palette = "auto",
extra = 104, # show fitted class, probs, percentages
branch.lty = 3, # dotted branch lines
shadow.col = "gray", # shadows under the node boxes
nn = TRUE, # display node numbers
cex = 0.7)   # -----NOT RENDERING TO PDF: HALTS EXECUTION

```


\newpage
## Fitting the Model

A 10- fold cross validation is set up, and the model fit on the trainig set. Number of possible cp values to evaluate is set to 10. 
When accuracy vs different values of cp is plotted below, it confirms previous findings. Raising the complexity parameter results in lower accuracy. The model is fit to the test data and model accuracy is 0.996, a high rate of precision in classifiying this species of mushrooms as poisonous or edible. 


```{r modelFit, echo = FALSE}
# Fit the model on the training set
set.seed(123)

model2 <- train(toxicity~., data = train.data, method = "rpart",
                trControl = trainControl("cv", number = 10),
                tuneLength = 10
                )

# Plot model accuracy vs different values of cp (complexity parameter)
plot(model2)

```
``` {r confMatrix, echo = FALSE}

# Make predictions on the test data
predicted.classes <- model2 %>% predict(test.data)

# Compute model accuracy rate on test data
mean(predicted.classes == test.data$toxicity)

# confusionMatrix(predicted.classes, test.data$toxicity) - BOTH need to be factorised
```
*Issue with Confusion Matrix: please see code*

# Building the Model: K Nearest Neighbour

K Nearest Neighbour (KNN) is a classification model that uses euclidean distance to calculate feature similarity. Using two points on a plane it measures the distance between plotted points and classifies accordingly. The model is a supervised machine learning algorithm. To ensure the data remains unbiased, a process of normalisation is usually applied to numeric data. 

## Prepping the Data

The data is not normalised to remove bias as the categories have been factorised and encoded as binary values.
```{r normal, echo = FALSE}
# # Normalising   ------NOT SURE I NEED TO DO THIS ON BINARY VALUES?
# # normalise function
# normalize <- function(x) {
# return ((x - min(x)) / (max(x) - min(x)))
# }
# 
# normalDataKNN <- as.data.frame(lapply(encodedDF2[2:118], normalize))
# 
# # test normalisation
# pander(summary(normalDataKNN$cap_shape.c))

```

## Building the Model 

Training and building the KNN model is performed in a single step. Initialisation of the *k* value is calculated as the square root of the number of observations. This is `r dfDim[1]`. The square root of 8124 is 90.13, so *k* will be set to 91, an odd number to avoid ties between two points in the KNN training process. 

```{r observ, echo = FALSE}
#Find the number of observation

nRows <- NROW(encodedDF2) 
#sqrt(nRows) 

```


```{r trainKNN, echo = FALSE}

# create test and train sets
train.KNN <- encodedDF2[1:6499,]
test.KNN <- encodedDF2[6500:8124,]

# apply labels 
train_labels <- encodedDF2[1:6499, 1] # creating labels from toxicity factor in col 1, 
test_labels <- encodedDF2[6500:8124, 1]

# trainLabels <- names(encodedDF2)
# cl = trainLabels


#knnModel1 <- knn(train=train.KNN, test=test.KNN, cl = train_labels, k=91)

# dim(train.data)
# 
# str(encodedDF2)
# 
# dim(train.dataKNN)
# dim(test.dataKNN)
# length(cl)


str(train_labels)
```

## Evaluating the Model 

Evaluating the model requires evaluating the number of true predicted classes. This can be tested and results seen below. 

```{r evalKNN, echo = FALSE}

# CrossTable(x = test_labels, y = knnModel1,
# prop.chisq=FALSE)

```

## Prediction


# Conclusion

This work attempts the binary classification of mushrooms according to edible or poisonous using two different classification models. The Decision Tree model performs with good accuracy. No improvements are reached through manual cp selection. The model appears to be successful though Confusion Matrix remains to be compiled to verify. The KNN model fails at the end: too many ties in knn. This means there are too many neighbors equidistant to the target point. Discrete values due to encoding the factors could be at fault. 

